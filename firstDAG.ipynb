{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.operators.email import EmailOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "\n",
        "CON = sqlite3.connect('example.db')\n",
        "\n",
        "# extrect from site\n",
        "def extract_data(url, tmp_file, **context) -> pd.DataFrame:\n",
        "    \"\"\" Extract CSV\n",
        "    \"\"\"\n",
        "    return pd.read_csv(url).to_csv(tmp_file)\n",
        "\n",
        "# group data\n",
        "def transform_data(group, agreg, tmp_file, tmp_file_agg, **context) -> None:\n",
        "    \"\"\" Group by data\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(tmp_file)\n",
        "    data.groupby(group).agg(agreg).reset_index().to_csv(tmp_file_agg)\n",
        "\n",
        "# load to db\n",
        "def load_data(tmp_file_agg, table_name, conn=CON, **context) -> None:\n",
        "    \"\"\" Load to DB\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(tmp_file_agg)\n",
        "    data[\"insert_time\"] = pd.to_datetime(\"now\")\n",
        "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "\n",
        "# DAG\n",
        "dag = DAG(dag_id='dag',\n",
        "          default_args={'owner':'airflow'},\n",
        "          schedule_interval='@daily',\n",
        "          start_date=days_ago(1))\n",
        "\n",
        "extract_data = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag,\n",
        "    op_kwargs={\n",
        "        'url': 'https://raw.githubusercontent.com/dm-novikov/stepik_airflow_course/main/data/data.csv',\n",
        "        'tmp_file': '/tmp/file.csv'\n",
        "    }\n",
        ")\n",
        "\n",
        "transform_data = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    dag=dag,\n",
        "    op_kwargs={\n",
        "        'tmp_file': '/tmp/file.csv',\n",
        "        'tmp_file_agg': '/tmp/file_agg.csv',\n",
        "        'group': ['A', 'B', 'C'],\n",
        "        'agreg': {'D': sum}}\n",
        ")\n",
        "\n",
        "load_data = PythonOperator(\n",
        "    task_id='load_data',\n",
        "    python_callable=load_data,\n",
        "    dag=dag,\n",
        "    op_kwargs={\n",
        "        'tmp_file_agg': '/tmp/file_agg.csv',\n",
        "        'table_name': 'table'\n",
        "    }\n",
        ")\n",
        "\n",
        "email_op = EmailOperator(\n",
        "    task_id='send_mail',\n",
        "    to=\"develtomas@gmail.com\",\n",
        "    subject='test mail',\n",
        "    html_content=\"\"\" mail tyt \"\"\",\n",
        "    files=['/tmp/file_agg.csv'],\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "extract_data >> transform_data >> [load_data, email_op]"
      ],
      "metadata": {
        "id": "m6VdGBIluDRn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}