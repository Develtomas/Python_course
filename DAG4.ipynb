{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPai+HnRR9+Nxo9KBlDfVa9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmbvTpj_Z35E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "from airflow import DAG\n",
        "from datetime import datetime\n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.operators.python import PythonOperator\n",
        "\n",
        "#connect to DB\n",
        "CON = sqlite3.connect('example.db')\n",
        "# const\n",
        "start = datetime(2021, 1, 1)\n",
        "end = datetime(2021, 1, 4)\n",
        "\n",
        "# extract currency from site\n",
        "def extract_rate(date, **context) -> None:\n",
        "  \"\"\" Extract currency rate from exchange\"\"\"\n",
        "  base = 'EUR'\n",
        "  symbols = 'USD'\n",
        "  format = 'csv'\n",
        "  url = f'https://api.exchangerate.host/timeseries?start_date={date}&end_date={date}&base={base}&symbols={symbols}&format={format}'\n",
        "  return pd.read_csv(url)['rate'][0]\n",
        "  \n",
        "\n",
        "# extract data from GIT\n",
        "def extract_data(date, data_file, **context) -> None:\n",
        "  \"\"\"date in string format yyyy-mm-dd\"\"\"\n",
        "  url = f'https://raw.githubusercontent.com/dm-novikov/stepik_airflow_course/main/data_new/{date}.csv'\n",
        "  \n",
        "  pd.read_csv(url).to_csv(data_file)\n",
        "\n",
        "\n",
        "# load to db\n",
        "def load_data(conn, data_file, **context) -> None:\n",
        "    \"\"\" Load to DB \"\"\"\n",
        "    data = pd.read_csv(data_file, usecols=['currency',\t'value',\t'date'])\n",
        "    data.to_sql('cur_table', conn, if_exists='append', index=False)\n",
        "\n",
        "\n",
        "# DAG\n",
        "dag = DAG(dag_id='dag',\n",
        "          default_args={'owner':'airflow'},\n",
        "          schedule_interval='@daily',\n",
        "          start_date=start,\n",
        "          end_date=end)\n",
        "\n",
        "extract_rate = PythonOperator(\n",
        "    task_id='extract_rate',\n",
        "    python_callable=extract_rate,\n",
        "    dag=dag,\n",
        "    do_xcom_push=True,\n",
        "    op_kwargs={\n",
        "      'date': '{{ ds }}'\n",
        "    }\n",
        ")\n",
        "\n",
        "extract_data = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag,\n",
        "    op_kwargs={\n",
        "      'date': '{{ ds }}',\n",
        "      'data_file': '/tmp/data.csv'\n",
        "    }\n",
        ")\n",
        "\n",
        "load_data = PythonOperator(\n",
        "    task_id='load_data',\n",
        "    python_callable=load_data,\n",
        "    dag=dag,\n",
        "    op_kwargs={\n",
        "      'data_file': '/tmp/data.csv',\n",
        "      'conn': CON\n",
        "    }\n",
        ")\n",
        "\n",
        "# sequence\n",
        "[extract_rate, extract_data] >> load_data"
      ]
    }
  ]
}